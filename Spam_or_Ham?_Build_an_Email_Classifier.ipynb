{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wakhidrf/Information-Security-Portfolio/blob/main/Spam_or_Ham%3F_Build_an_Email_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1df29eda",
      "metadata": {
        "id": "1df29eda"
      },
      "source": [
        "Step 0. Go to the Ham or Spam dataset [website](http://www2.aueb.gr/users/ion/data/enron-spam/index.html) and download the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f392b2d4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f392b2d4",
        "outputId": "6f172907-bdac-43ce-9bf7-efef869cfeca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-27 00:33:47--  https://cdn.theforage.com/vinternships/companyassets/Sj7temL583QAYpHXD/JiwEkbBq8pFwMRYLc/1635444722719/enron1.zip\n",
            "Resolving cdn.theforage.com (cdn.theforage.com)... 18.65.39.85, 18.65.39.31, 18.65.39.102, ...\n",
            "Connecting to cdn.theforage.com (cdn.theforage.com)|18.65.39.85|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5783105 (5.5M) [application/zip]\n",
            "Saving to: ‘enron1.zip’\n",
            "\n",
            "enron1.zip          100%[===================>]   5.51M  7.35MB/s    in 0.8s    \n",
            "\n",
            "2024-01-27 00:33:48 (7.35 MB/s) - ‘enron1.zip’ saved [5783105/5783105]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://cdn.theforage.com/vinternships/companyassets/Sj7temL583QAYpHXD/JiwEkbBq8pFwMRYLc/1635444722719/enron1.zip\n",
        "!unzip -qq enron1.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf32cfce",
      "metadata": {
        "id": "bf32cfce"
      },
      "source": [
        "Step 1. Traverse the dataset and create a Pandas dataframe. This is already done for you. You should recognize Pandas from task 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "20c5d195",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20c5d195",
        "outputId": "581da51b-603c-4543-b454-bdffe80f304f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipped 2140.2004-09-13.GP.spam.txt\n",
            "skipped 3304.2004-12-26.GP.spam.txt\n",
            "skipped 2042.2004-08-30.GP.spam.txt\n",
            "skipped 4142.2005-03-31.GP.spam.txt\n",
            "skipped 2526.2004-10-17.GP.spam.txt\n",
            "skipped 5105.2005-08-31.GP.spam.txt\n",
            "skipped 3364.2005-01-01.GP.spam.txt\n",
            "skipped 4350.2005-04-23.GP.spam.txt\n",
            "skipped 4201.2005-04-05.GP.spam.txt\n",
            "skipped 2248.2004-09-23.GP.spam.txt\n",
            "skipped 2649.2004-10-27.GP.spam.txt\n",
            "skipped 4566.2005-05-24.GP.spam.txt\n",
            "skipped 2698.2004-10-31.GP.spam.txt\n",
            "skipped 1414.2004-06-24.GP.spam.txt\n",
            "skipped 0754.2004-04-01.GP.spam.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-49292689b560>:31: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(pd.DataFrame.from_records(spam))\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def read_spam():\n",
        "    category = 'spam'\n",
        "    directory = './enron1/spam'\n",
        "    return read_category(category, directory)\n",
        "\n",
        "def read_ham():\n",
        "    category = 'ham'\n",
        "    directory = './enron1/ham'\n",
        "    return read_category(category, directory)\n",
        "\n",
        "def read_category(category, directory):\n",
        "    emails = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if not filename.endswith(\".txt\"):\n",
        "            continue\n",
        "        with open(os.path.join(directory, filename), 'r') as fp:\n",
        "            try:\n",
        "                content = fp.read()\n",
        "                emails.append({'name': filename, 'content': content, 'category': category})\n",
        "            except:\n",
        "                print(f'skipped {filename}')\n",
        "    return emails\n",
        "\n",
        "ham = read_ham()\n",
        "spam = read_spam()\n",
        "\n",
        "df = pd.DataFrame.from_records(ham)\n",
        "df = df.append(pd.DataFrame.from_records(spam))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a1c23fd",
      "metadata": {
        "id": "1a1c23fd"
      },
      "source": [
        "Step 2. Data cleaning is a critical part of machine learning. You and I can recognize that 'Hello' and 'hello' are the same word but a machine does not know this a priori. Therefore, we can 'help' the machine by conducting such normalization steps for it. Write a function `preprocessor` that takes in a string and replaces all non alphabet characters with a space and then lowercases the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c447c901",
      "metadata": {
        "id": "c447c901"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def preprocessor(e):\n",
        "    return re.sub('[^A-Za-z]', ' ', e).lower()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba32521d",
      "metadata": {
        "id": "ba32521d"
      },
      "source": [
        "Step 3. We will now train the machine learning model. All the functions that you will need are imported for you. The instructions explain how the work and hint at which functions to use. You will likely need to refer to the scikit learn documentation to see how exactly to invoke the functions. It will be handy to keep that tab open."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1442d377",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1442d377",
        "outputId": "b595f114-e367-4008-9d15-5a2856a087e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:\n",
            "0.9844961240310077\n",
            "\n",
            "Confusion Matrix:\n",
            "[[729  13]\n",
            " [  3 287]]\n",
            "\n",
            "Detailed Statistics:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       1.00      0.98      0.99       742\n",
            "        spam       0.96      0.99      0.97       290\n",
            "\n",
            "    accuracy                           0.98      1032\n",
            "   macro avg       0.98      0.99      0.98      1032\n",
            "weighted avg       0.98      0.98      0.98      1032\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# The CountVectorizer converts a text sample into a vector (think of it as an array of floats).\n",
        "# Each entry in the vector corresponds to a single word and the value is the number of times the word appeared.\n",
        "# Instantiate a CountVectorizer. Make sure to include the preprocessor you previously wrote in the constructor.\n",
        "vectorizer = CountVectorizer(preprocessor=preprocessor)\n",
        "\n",
        "# Use train_test_split to split the dataset into a train dataset and a test dataset.\n",
        "# The machine learning model learns from the train dataset.\n",
        "# Then the trained model is tested on the test dataset to see if it actually learned anything.\n",
        "# If it just memorized for example, then it would have a low accuracy on the test dataset and a high accuracy on the train dataset.\n",
        "X_train,X_test,y_train,y_test = train_test_split(df[\"content\"],df[\"category\"],test_size=0.2,random_state=1)\n",
        "\n",
        "# Use the vectorizer to transform the existing dataset into a form in which the model can learn from.\n",
        "# Remember that simple machine learning models operate on numbers, which the CountVectorizer conveniently helped us do.\n",
        "X_train_df = vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Use the LogisticRegression model to fit to the train dataset.\n",
        "# You may remember y = mx + b and Linear Regression from high school. Here, we fitted a scatter plot to a line.\n",
        "# Logistic Regression is another form of regression.\n",
        "# However, Logistic Regression helps us determine if a point should be in category A or B, which is a perfect fit.\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_df,y_train)\n",
        "\n",
        "# Validate that the model has learned something.\n",
        "# Recall the model operates on vectors. First transform the test set using the vectorizer.\n",
        "# Then generate the predictions.\n",
        "X_test_df = vectorizer.transform(X_test)\n",
        "y_pred = model.predict(X_test_df)\n",
        "\n",
        "# We now want to see how we have done. We will be using three functions.\n",
        "# `accuracy_score` tells us how well we have done.\n",
        "# 90% means that every 9 of 10 entries from the test dataset were predicted accurately.\n",
        "# The `confusion_matrix` is a 2x2 matrix that gives us more insight.\n",
        "# The top left shows us how many ham emails were predicted to be ham (that's good!).\n",
        "# The bottom right shows us how many spam emails were predicted to be spam (that's good!).\n",
        "# The other two quadrants tell us the misclassifications.\n",
        "# Finally, the `classification_report` gives us detailed statistics which you may have seen in a statistics class.\n",
        "print(f'Accuracy:\\n{accuracy_score(y_test,y_pred)}\\n')\n",
        "print(f'Confusion Matrix:\\n{confusion_matrix(y_test,y_pred)}\\n')\n",
        "print(f'Detailed Statistics:\\n{classification_report(y_test,y_pred)}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9674d032",
      "metadata": {
        "id": "9674d032"
      },
      "source": [
        "Step 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6b7d78c9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b7d78c9",
        "outputId": "13542902-812c-4c56-927f-ddc5809e93fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1.0044362456278242 http\n",
            "0.8176885681071172 prices\n",
            "0.7534654533601661 no\n",
            "0.7461598743726818 removed\n",
            "0.7267990653549244 paliourg\n",
            "0.7265242987677138 money\n",
            "0.713340948588752 pain\n",
            "0.7021934812555215 hello\n",
            "0.6604174958432891 only\n",
            "0.6464351702087976 here\n",
            "\n",
            "-1.5794746802145332 attached\n",
            "-1.5759056713601534 enron\n",
            "-1.485565636119017 daren\n",
            "-1.3598683323538934 thanks\n",
            "-1.2818984200245023 doc\n",
            "-1.227009256033395 pictures\n",
            "-1.1982884032602719 xls\n",
            "-1.172552906695079 neon\n",
            "-1.129481269928728 meter\n",
            "-1.1232421117631208 hpl\n"
          ]
        }
      ],
      "source": [
        "# Let's see which features (aka columns) the vectorizer created.\n",
        "# They should be all the words that were contained in the training dataset.\n",
        "features = vectorizer.get_feature_names_out()\n",
        "\n",
        "# You may be wondering what a machine learning model is tangibly. It is just a collection of numbers.\n",
        "# You can access these numbers known as \"coefficients\" from the coef_ property of the model\n",
        "# We will be looking at coef_[0] which represents the importance of each feature.\n",
        "# What does importance mean in this context?\n",
        "# Some words are more important than others for the model.\n",
        "# It's nothing personal, just that spam emails tend to contain some words more frequently.\n",
        "# This indicates to the model that having that word would make a new email more likely to be spam.\n",
        "importance = model.coef_[0]\n",
        "\n",
        "# Iterate over importance and find the top 10 positive features with the largest magnitude.\n",
        "# Similarly, find the top 10 negative features with the largest magnitude.\n",
        "# Positive features correspond to spam. Negative features correspond to ham.\n",
        "# You will see that `http` is the strongest feature that corresponds to spam emails.\n",
        "# It makes sense. Spam emails often want you to click on a link.\n",
        "l = list(enumerate(importance))\n",
        "print()\n",
        "l.sort(key=lambda e: e[1], reverse=True)\n",
        "for i,imp in l[:10]:\n",
        "    print(imp, features[i])\n",
        "print()\n",
        "l.sort(key=lambda e: -e[1], reverse=True)\n",
        "for i,imp in l[:10]:\n",
        "    print(imp, features[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d267e7ad",
      "metadata": {
        "id": "d267e7ad"
      },
      "source": [
        "All Done!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}